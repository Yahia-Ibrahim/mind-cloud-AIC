{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxBZ794JaDdM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he4T4ZIZaDdN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BRXWCZJaDdO"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "ejlcRWOuaDdP",
        "outputId": "9e15e8c4-1b95-4755-f562-d0c66c87a95e"
      },
      "outputs": [],
      "source": [
        "data_path = \"train_adapt\"   # put the path to the dataset\n",
        "wavs_path = data_path + \"/train/\"\n",
        "metadata_path = data_path + \"/train.csv\"\n",
        "\n",
        "validation_path = data_path + \"/adapt/\"\n",
        "validation_metadata_path = data_path + \"/adapt.csv\"\n",
        "\n",
        "\n",
        "# Read metadata file and parse it\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\",\", header=0, quoting=3)\n",
        "metadata_df.columns = [\"audio\", \"transcript\"]\n",
        "metadata_df = metadata_df[[\"audio\", \"transcript\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "validation_metadata_df = pd.read_csv(validation_metadata_path, sep=\",\", header=0, quoting=3)\n",
        "validation_metadata_df.columns = [\"audio\", \"transcript\"]\n",
        "validation_metadata_df = validation_metadata_df[[\"audio\", \"transcript\"]]\n",
        "validation_metadata_df = validation_metadata_df.sample(frac=1).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "oqg4WUy-nokc",
        "outputId": "9dcef120-1088-4ea7-d7e7-f29a4485b4fd"
      },
      "outputs": [],
      "source": [
        "validation_metadata_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoNJGhG4aDdQ"
      },
      "source": [
        "We now split the data into training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkz3Nep0aDdQ",
        "outputId": "dc69dec6-6eb8-42e2-da1b-0e4434056a04"
      },
      "outputs": [],
      "source": [
        "df_train = metadata_df\n",
        "df_val = validation_metadata_df\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the validation set: {len(df_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X1cek8kaDdR"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We first prepare the vocabulary to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snet44lfaDdT",
        "outputId": "b43ecfc4-bcf6-4a1f-af17-b269a959202e"
      },
      "outputs": [],
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"غظضذخثةتشقرصفعسمنلكيطحزوؤهدجبىائءإآأ \"]\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmvYbUEXaDdT"
      },
      "source": [
        "Next, we create the function that describes the transformation that we apply to each\n",
        "element of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzEJdqc4aDdU"
      },
      "outputs": [],
      "source": [
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 240 \n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 120 \n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length. /////////////////////////////\n",
        "\n",
        "fft_length = 256 \n",
        "\n",
        "sample_rate = 16000\n",
        "def encode_single_sample_train(wav_file, label):\n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    stfts = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(stfts)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    ###########################################\n",
        "    ##  Process the label\n",
        "    ##########################################\n",
        "    # 7. Split the label\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    # 8. Map the characters in label to numbers\n",
        "    label = char_to_num(label)\n",
        "    # 10. Return a dict as our model is expecting two inputs\n",
        "    return spectrogram, label   #spectrogram\n",
        "\n",
        "\n",
        "def encode_single_sample_validation(wav_file, label):\n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(validation_path + wav_file + \".wav\")\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    stfts = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(stfts)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    ###########################################\n",
        "    ##  Process the label\n",
        "    ##########################################\n",
        "    # 7. Split the label\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    # 8. Map the characters in label to numbers\n",
        "    label = char_to_num(label)\n",
        "    # 10. Return a dict as our model is expecting two inputs\n",
        "    return spectrogram, label   #spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNjPEsezaDdU"
      },
      "source": [
        "## Creating `Dataset` objects\n",
        "\n",
        "We create a `tf.data.Dataset` object that yields\n",
        "the transformed elements, in the same order as they\n",
        "appeared in the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C771TE9JaDdV",
        "outputId": "5b536b80-8ebc-4a72-e060-01cf7a03144a"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "print(len(list(df_train[\"audio\"])))\n",
        "print(len(list(df_train[\"transcript\"])))\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (np.array(df_train[\"audio\"].tolist()), np.array(df_train[\"transcript\"].tolist()))\n",
        ")\n",
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (np.array(df_val[\"audio\"].tolist()), np.array(df_val[\"transcript\"].tolist()))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample_validation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaHYzyEyaDdZ"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Let's visualize an example in our dataset, including the\n",
        "audio clip, the spectrogram and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "lpTb_aJeaDda",
        "outputId": "7c5c19fc-f2cd-4cad-85bd-bbcc1d434ccd"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 5))\n",
        "for batch in validation_dataset.take(1):\n",
        "    spectrogram = batch[0][0].numpy()\n",
        "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "    label = batch[1][0]\n",
        "    # Spectrogram\n",
        "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    ax.imshow(spectrogram, vmax=1)\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "    # Wav\n",
        "    file = tf.io.read_file(validation_path + list(df_val[\"audio\"])[0] + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = audio.numpy()\n",
        "    ax = plt.subplot(2, 1, 2)\n",
        "    plt.plot(audio)\n",
        "    ax.set_title(\"Signal Wave\")\n",
        "    ax.set_xlim(0, len(audio))\n",
        "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4BQyb6taDda"
      },
      "source": [
        "## Model\n",
        "\n",
        "We first define the CTC Loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BBrp4W9aDdb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqiccPRjaDdb"
      },
      "source": [
        "We now define our model. We will define a model similar to\n",
        "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkSvWCYdaDdc",
        "outputId": "03c208a4-0e8d-4f86-ba14-9e802a2e4871"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_model(input_dim, output_dim, rnn_layers= 5, rnn_units=256):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "\n",
        "     # Convolutional layers\n",
        "    for i, (filters, kernel_size, strides) in enumerate(\n",
        "        [(96, [11, 41], [2, 2]), (128, [11, 21], [1, 2])\n",
        "        ]\n",
        "    ):\n",
        "        x = layers.Conv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding=\"same\",\n",
        "            use_bias=False,\n",
        "            name=f\"conv_{i+1}\",\n",
        "            kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "        )(x)\n",
        "        x = layers.ReLU(name=f\"conv_{i+1}_relu\")(x)\n",
        "\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "            kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-7)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=768,\n",
        ")\n",
        "model.summary(line_length=110)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M95bpHbiaDdd"
      },
      "outputs": [],
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True, beam_width=512)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "            break\n",
        "\n",
        "        for i in np.random.randint(0, len(predictions), 32):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVeeDL63aDdd"
      },
      "source": [
        "Let's start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xgzt1rvrxHtb",
        "outputId": "6d7d88fb-e686-471e-fc1a-6f78ea3a7dcf"
      },
      "outputs": [],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1k4VYLAkZhs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpf7g0cvaDde",
        "outputId": "170eb5fd-8aae-4f81-dfc7-25862d696026"
      },
      "outputs": [],
      "source": [
        "# Define the checkpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='milstones/model_{epoch:02d}_{val_loss:.2f}.h5',  # Path to save the model\n",
        "    save_weights_only=True,  # Only save the model's weights\n",
        "    save_freq= \"epoch\",  # Save after each epoch\n",
        "    verbose=1  # Print a message when saving the model\n",
        ")\n",
        "\n",
        "# Define the number of epochs.\n",
        "epochs = 1\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[checkpoint_callback , validation_callback]\n",
        "  )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
